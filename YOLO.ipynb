{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "882bfb6a",
   "metadata": {},
   "source": [
    "# Modell 2 mit pretrained YOLO\n",
    "\n",
    "## Theory\n",
    " - Understanding YOLOv8: https://medium.com/@melissa.colin/yolov8-explained-understanding-object-detection-from-scratch-763479652312\n",
    " - Slightly more detailed: https://medium.com/@vindyalenawala/yolov8-architecture-a-detailed-overview-5e2c371cf82a\n",
    " - Architecture of YOLOv8: https://arxiv.org/html/2408.15857v1#:~:text=The%20architecture%20of%20YOLOv8%20is%20structured%20around,minimize%20computational%20overhead%20while%20retaining%20representational%20power.\n",
    " - CSPNet: https://arxiv.org/pdf/1911.11929v1\n",
    " - offizielle YOLOv8: https://yolov8.org/yolov8-architecture/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c7a283",
   "metadata": {},
   "source": [
    "## Flood Area Segmentation mit YOLO\n",
    "\n",
    "Wir haben den Code in Kaggle gefunden: https://www.kaggle.com/code/myoungjinson/flood-area-segmentation/notebook#YOLOv8\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e216a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ.pop('MPLBACKEND', None)\n",
    "import cv2\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295e72c8",
   "metadata": {},
   "source": [
    "Daten laden (gleiche Daten wie in U-Net Modell von Kaggle) \n",
    "\n",
    "https://www.youtube.com/watch?v=diZj_nPVLkE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b1a8b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "from pathlib import Path\n",
    "\n",
    "# collect data\n",
    "path = kagglehub.dataset_download(\"faizalkarim/flood-area-segmentation\")\n",
    "base_dir = Path(path)\n",
    "\n",
    "input_dir_masks = os.listdir(os.path.join(base_dir, \"Mask\"))\n",
    "input_dir_images = os.listdir(os.path.join(base_dir, \"Image\"))\n",
    "\n",
    "#convert Mask into label for YOLOv8\n",
    "#target folder to create labels\n",
    "output_dir_labels = os.path.join(base_dir, \"labels\")\n",
    "if not os.path.exists(output_dir_labels):\n",
    "    os.makedirs(output_dir_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdad78e",
   "metadata": {},
   "source": [
    "#### Datenvorbereitung\n",
    "\n",
    "Create lables from masks -> polygons (contours) in txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ee31b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in os.listdir(input_dir_masks):\n",
    "    image_path = os.path.join(input_dir_masks, j)\n",
    "    # load the binary mask and get its contours\n",
    "    mask = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    # convert mask in pure black and white images\n",
    "    _, mask = cv2.threshold(mask, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    H, W = mask.shape\n",
    "    #find the boundaries of the white area and the outlines  \n",
    "    contours, hiearchy = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # convert the contours to polygons\n",
    "    polygons = []\n",
    "    for cnt in contours:\n",
    "        if cv2.contourArea(cnt) > 200:\n",
    "            polygon = []\n",
    "            for point in cnt:\n",
    "                x, y = point [0]\n",
    "                polygon.append(x / W)\n",
    "                polygon.append(y / H)\n",
    "            polygons.append(polygon)\n",
    "\n",
    "    # print the polygons\n",
    "    with open('{}.txt'.format(os.path.join(output_dir_labels, j)[:-4]), 'w') as f:\n",
    "        for polygon in polygons:\n",
    "            for p_, p in enumerate(polygon):\n",
    "                if p_ == len(polygon) - 1:\n",
    "                    f.write('{}\\n'.format(p))\n",
    "                elif p_ == 0:\n",
    "                    f.write('0 {} '.format(p))\n",
    "                else:\n",
    "                    f.write('{} '.format(p))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7483e4",
   "metadata": {},
   "source": [
    "Split in Train, Validation und Test - wird von U-Net Datenvorbereitung übernommen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0ccfbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images: 232\n",
      "Train masks:  435\n",
      "Val images:   74\n",
      "Val masks:    101\n",
      "Test images:  44\n",
      "Test masks:   44\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "\n",
    "train_imgs, temp_imgs, train_masks, temp_masks = train_test_split(\n",
    "    img_paths, mask_paths,\n",
    "    train_size=0.7,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_imgs, test_imgs, val_masks, test_masks = train_test_split(\n",
    "    temp_imgs, temp_masks,\n",
    "    train_size=0.5,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# copy in directories\n",
    "def copy_subset(img_list, mask_list, subset):\n",
    "    for img, mask in zip(img_list, mask_list):\n",
    "        shutil.copy(img,  f\"dataset/images/{subset}/{img.name}\")\n",
    "        shutil.copy(mask, f\"dataset/labels/{subset}/{mask.stem}.png\")\n",
    "\n",
    "copy_subset(train_imgs, train_masks, 'train')\n",
    "copy_subset(val_imgs,   val_masks,   'val')\n",
    "copy_subset(test_imgs,  test_masks,  'test')\n",
    "\n",
    "# 7) Kontrolle\n",
    "print(\"Train images:\", len(os.listdir(\"dataset/images/train\")))\n",
    "print(\"Train masks: \", len(os.listdir(\"dataset/labels/train\")))\n",
    "print(\"Val images:  \", len(os.listdir(\"dataset/images/val\")))\n",
    "print(\"Val masks:   \", len(os.listdir(\"dataset/labels/val\")))\n",
    "print(\"Test images: \", len(os.listdir(\"dataset/images/test\")))\n",
    "print(\"Test masks:  \", len(os.listdir(\"dataset/labels/test\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2da4ce",
   "metadata": {},
   "source": [
    "Masken müssen wir für YOLO in text files umwandeln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc9473e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "example_mask = Image.open(\"dataset/labels/train/0.png\")\n",
    "plt.imshow(example_mask, cmap='gray')\n",
    "plt.title(\"Sample Mask\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9457c5b",
   "metadata": {},
   "source": [
    "## YOLOv8\n",
    "\n",
    "https://medium.com/@melissa.colin/yolov8-explained-understanding-object-detection-from-scratch-763479652312\n",
    "\n",
    "steht für \"You Only Look Once\" -> für schnelle Inferenzzeiten\n",
    "\n",
    "wird hier für Segementierung und Objektlokalisierung verwendet -> YOLOv8 ermöglicht Pixel-genaue Masken vorhersagen (vergleichbar mit U-NET, aber zusätzlich mit Objektdetektion)\n",
    "\n",
    "trainiert wird mit der Library ultralytics; außerdem wird hier ein vortrainiertes Modell geladen und verwendet (verfeinert) -> YOLO(\"yolo11n-seg.yaml\").load(\"yolo11n.pt\")\n",
    "\n",
    "### Architektur\n",
    "\n",
    "![U-Net Architektur](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*frzqTxCGA8DHEHMeHug7JQ.png)\n",
    "\n",
    "#### Backbone – Merkmalsextraktion\n",
    "\n",
    "Das Backbone extrahiert grundlegende Merkmale wie Kanten, Formen und Texturen aus dem Eingabebild. YOLOv8 verwendet eine modifizierte CSPDarknet53-Architektur, die folgende Techniken kombiniert:\n",
    "\n",
    "- Convolutional Layers: für Merkmalsextraktion\n",
    "- Residual Blocks: verhindern den Informationsverlust in tiefen Netzen\n",
    "- CSPNet (Cross Stage Partial Networks): reduziert Rechenaufwand und verbessert den Gradientendurchfluss\n",
    "- Darknet53 / CSPDarknet53: als Grundlage des Feature-Backbones\n",
    "\n",
    "#### Neck – Merkmalsfusion auf mehreren Skalen\n",
    "\n",
    "Der Neck kombiniert Merkmale unterschiedlicher Auflösung (also unterschiedlicher Conv Layer zB) und verwendet:\n",
    "\n",
    "- FPN (Feature Pyramid Networks): verarbeitet Bildinformationen auf verschiedenen Skalen\n",
    "- PANet (Path Aggregation Network): verbessert den Informationsfluss zwischen Ebenen, besonders für kleinere Objekte\n",
    "\n",
    "Die Feature-Maps (zB P3, P4, P5) werden auf verschiedenen Ebenen zusammengeführt, um die Erkennung von Objekten unterschiedlicher Größe zu ermöglichen.\n",
    "\n",
    "#### Head – Erkennung & Klassifikation\n",
    "\n",
    "Der Head erzeugt:\n",
    "\n",
    "- Bounding Boxes\n",
    "- Konfidenzwerte\n",
    "- Klassenvorhersagen\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2a5215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mask_to_yolo_txt(mask_path, txt_path, class_id=0):\n",
    "    mask = cv2.imread(str(mask_path), 0)\n",
    "    if mask is None:\n",
    "        print(f\"Failed to read {mask_path}\")\n",
    "        return False\n",
    "\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        print(f\"No contours found in {mask_path}\")\n",
    "        return False\n",
    "    h, w = mask.shape\n",
    "\n",
    "    with open(txt_path, \"w\") as f:\n",
    "        for contour in contours:\n",
    "            if len(contour) < 3:\n",
    "                continue\n",
    "            contour = contour.squeeze()\n",
    "            norm = contour / [w, h]\n",
    "            norm = norm.flatten()\n",
    "            norm = ' '.join(map(str, norm))\n",
    "            f.write(f\"{class_id} {norm}\\n\")\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "for subset in ['train', 'val']:\n",
    "    mask_dir = Path(f\"dataset/labels/{subset}\")\n",
    "    for mask_path in mask_dir.glob(\"*.png\"):\n",
    "        txt_path = mask_path.with_suffix(\".txt\")  # same folder, same name\n",
    "        success = mask_to_yolo_txt(mask_path, txt_path)\n",
    "        if success:\n",
    "            os.remove(mask_path)  # only remove PNG if txt was created\n",
    "        else:\n",
    "            if txt_path.exists():\n",
    "                os.remove(txt_path)  # remove empty or invalid txt\n",
    "\n",
    "\n",
    "example_txts = list(Path(\"dataset/labels/val\").glob(\"*.txt\"))\n",
    "if example_txts:\n",
    "    print(f\"\\nSuccessfully created: {example_txts[0]}\")\n",
    "    with open(example_txts[0], \"r\") as f:\n",
    "        print(\"Example content:\\n\", f.read())\n",
    "else:\n",
    "    print(\"No YOLO label files found.\")\n",
    "\n",
    "\n",
    "print(\"Train labels:\", len(os.listdir(\"dataset/labels/train\")))\n",
    "print(\"Val labels:  \", len(os.listdir(\"dataset/labels/val\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d134ee4",
   "metadata": {},
   "source": [
    "jetzt ein data.yaml erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55327529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "data_yaml = {\n",
    "    'path': 'dataset',                  # root directory\n",
    "    'train': 'images/train',            # relative to dataset/\n",
    "    'val': 'images/val',                # relative to dataset/\n",
    "    'nc': 1,                            # number of classes\n",
    "    'names': ['flood']                  # list of class names\n",
    "}\n",
    "\n",
    "with open('data.yaml', 'w') as f:\n",
    "    yaml.dump(data_yaml, f)\n",
    "\n",
    "!cat data.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0ec4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"yolo11n-seg.yaml\")  # build a new model from YAML\n",
    "model = YOLO(\"yolo11n-seg.pt\")  # load a pretrained model (recommended for training)\n",
    "model = YOLO(\"yolo11n-seg.yaml\").load(\"yolo11n.pt\")  # build from YAML and transfer weights\n",
    "\n",
    "# Train the model with coco8-seg.yaml\n",
    "results = model.train(data=\"data.yaml\", epochs=100, imgsz=640, batch=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449a6962",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d65cead",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c536b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b2c7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb49395",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d33b898",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.model.to(device)\n",
    "\n",
    "summary(model.model, input_size=(1, 3, 640, 640), col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d2214c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. load image\n",
    "val_image_path = \"dataset/images/val/1002.jpg\"\n",
    "img = cv2.imread(val_image_path)\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# 2. load mask\n",
    "path = kagglehub.dataset_download(\"faizalkarim/flood-area-segmentation\")\n",
    "base_dir = Path(path)\n",
    "mask_filename = \"1002.png\"\n",
    "mask_path = base_dir / \"Mask\" / mask_filename\n",
    "mask = Image.open(mask_path).convert(\"L\")\n",
    "mask = np.array(mask)\n",
    "\n",
    "# 3. prediction of model\n",
    "results = model.predict(source=val_image_path, save=False)\n",
    "pred_mask = results[0].masks.data[0].cpu().numpy()\n",
    "\n",
    "# 4. plot\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Original Image\")\n",
    "plt.imshow(img_rgb)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Ground Truth Mask\")\n",
    "plt.imshow(mask, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Predicted Mask\")\n",
    "plt.imshow(pred_mask, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa855a4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
